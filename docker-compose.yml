services:
  # =============================================================================
  # KAFKA - Message Broker
  # =============================================================================
  kafka:
    # Use official Apache Kafka image
    image: apache/kafka:3.7.0

    # Name the container "kafka" (easier to reference in logs/commands)
    container_name: kafka

    # Expose ports to host machine
    # Format: "HOST_PORT:CONTAINER_PORT"
    ports:
      - "9092:9092"    # External access (from your machine)

    environment:
      # ============================================
      # KRaft Mode Configuration (Kafka without Zookeeper)
      # ============================================
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093

      # ============================================
      # Listener Configuration (IMPORTANT FOR DOCKER)
      # ============================================
      # We need TWO listeners:
      # - INTERNAL: for Docker containers (Spark) to connect
      # - EXTERNAL: for your machine (producer.py) to connect

      # Define all listeners Kafka will bind to
      # Format: LISTENER_NAME://HOST:PORT
      KAFKA_LISTENERS: INTERNAL://:29092,EXTERNAL://:9092,CONTROLLER://:9093

      # What addresses to advertise to clients
      # - INTERNAL: other containers use "kafka:29092" (Docker DNS)
      # - EXTERNAL: your machine uses "localhost:9092"
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092

      # Security protocol for each listener
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT

      # Which listener for controller (internal Kafka communication)
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # Which listener for inter-broker communication
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      # ============================================
      # Topic Configuration (single-node setup)
      # ============================================
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

    # Healthcheck to know when Kafka is ready
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5

  # =============================================================================
  # SPARK - Stream Processing
  # =============================================================================
  spark:
    # Official Apache Spark image (includes PySpark)
    image: apache/spark:3.5.0

    # Name the container
    container_name: spark

    # Run as root to allow VS Code/Cursor to install its server
    user: root

    # Wait for Kafka to be ready before starting
    depends_on:
      kafka:
        condition: service_healthy

    # Mount our code into the container
    # Format: "HOST_PATH:CONTAINER_PATH"
    volumes:
      - ./:/app    # Current folder -> /app in container

    # Set working directory inside container
    working_dir: /app

    environment:
      # ============================================
      # Spark Configuration
      # ============================================
      # Run Spark in local mode with 2 threads
      # Options: local[1], local[2], local[*] (all cores)
      SPARK_MASTER: local[2]

    # Keep container running so we can exec into it
    # (We'll manually run spark_consumer.py)
    command: sleep infinity
