services:
  # =============================================================================
  # INFRASTRUCTURE (always start, no profile)
  # =============================================================================

  # Kafka - Message Broker
  kafka:
    image: apache/kafka:3.7.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      # KRaft Mode (no Zookeeper)
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093

      # Listeners: INTERNAL (Docker) + EXTERNAL (host) + CONTROLLER
      KAFKA_LISTENERS: INTERNAL://:29092,EXTERNAL://:9092,CONTROLLER://:9093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      # Single-node topic defaults
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ClickHouse - Analytics Database
  clickhouse:
    image: clickhouse/clickhouse-server:24.1
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
    environment:
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # =============================================================================
  # PRODUCERS
  # =============================================================================

  # Synthetic stock data producer (demo)
  stock-producer:
    build:
      context: ./src/demo
      dockerfile: Dockerfile
    container_name: stock-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
    profiles:
      - synthetic

  # Real-time crypto producer (Coinbase WebSocket)
  crypto-producer:
    build:
      context: ./src/production/producer
    container_name: crypto-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
    profiles:
      - crypto

  # =============================================================================
  # CONSUMERS
  # =============================================================================

  # Spark micro-batch consumer -> ClickHouse
  spark-microbatch:
    build:
      context: ./src/production/consumer
      dockerfile: Dockerfile.spark
    container_name: spark-microbatch
    depends_on:
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: "8123"
    profiles:
      - spark-microbatch

  # Spark streaming consumer (windowed aggregations) -> ClickHouse
  spark-streaming:
    build:
      context: ./src/production/consumer
      dockerfile: Dockerfile.spark
    container_name: spark-streaming
    depends_on:
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: "8123"
      CHECKPOINT_DIR_RAW: /checkpoints/raw_ticks
      CHECKPOINT_DIR_AGG: /checkpoints/windowed_agg
    volumes:
      - spark_checkpoints:/checkpoints
    command: >
      /opt/spark/bin/spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.clickhouse:clickhouse-jdbc:0.6.0,org.apache.httpcomponents.client5:httpclient5:5.3.1
      spark_streaming_clickhouse_consumer.py
    profiles:
      - spark-streaming

  # Flink consumer -> ClickHouse
  flink-consumer:
    build:
      context: .
      dockerfile: src/production/consumer/Dockerfile.flink
    container_name: flink-consumer
    depends_on:
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: "8123"
    profiles:
      - flink

  # =============================================================================
  # DASHBOARDS
  # =============================================================================

  # Streamlit dashboard (port 8501) - for synthetic/stock data only
  streamlit:
    build:
      context: ./src/production/dashboard
    container_name: streamlit
    depends_on:
      clickhouse:
        condition: service_healthy
    environment:
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: "8123"
    ports:
      - "8501:8501"
    profiles:
      - streamlit

  # FastAPI web dashboard (port 8502) - works with both stock and crypto
  web-dashboard:
    build:
      context: ./src/production/dashboard
    container_name: web-dashboard
    depends_on:
      clickhouse:
        condition: service_healthy
    environment:
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: "8123"
      DASHBOARD_MODE: ${DASHBOARD_MODE:-stock}
    ports:
      - "8502:8502"
    command: ["python", "web_app.py"]
    profiles:
      - web

  # =============================================================================
  # DEMO
  # =============================================================================

  # Demo Spark consumer (console output only, no ClickHouse)
  spark-demo:
    build:
      context: ./src/demo
      dockerfile: Dockerfile.spark-demo
    container_name: spark-demo
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
    profiles:
      - demo

# =============================================================================
# VOLUMES
# =============================================================================
volumes:
  clickhouse_data:
  spark_checkpoints:
