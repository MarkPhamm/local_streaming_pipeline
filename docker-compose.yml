services:
  # =============================================================================
  # KAFKA - Message Broker
  # =============================================================================
  kafka:
    # Use official Apache Kafka image
    image: apache/kafka:3.7.0

    # Name the container "kafka" (easier to reference in logs/commands)
    container_name: kafka

    # Expose ports to host machine
    # Format: "HOST_PORT:CONTAINER_PORT"
    ports:
      - "9092:9092"    # External access (from your machine)

    environment:
      # ============================================
      # KRaft Mode Configuration (Kafka without Zookeeper)
      # ============================================
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093

      # ============================================
      # Listener Configuration (IMPORTANT FOR DOCKER)
      # ============================================
      # We need TWO listeners:
      # - INTERNAL: for Docker containers (Spark) to connect
      # - EXTERNAL: for your machine (stock_producer.py) to connect

      # Define all listeners Kafka will bind to
      # Format: LISTENER_NAME://HOST:PORT
      KAFKA_LISTENERS: INTERNAL://:29092,EXTERNAL://:9092,CONTROLLER://:9093

      # What addresses to advertise to clients
      # - INTERNAL: other containers use "kafka:29092" (Docker DNS)
      # - EXTERNAL: your machine uses "localhost:9092"
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092

      # Security protocol for each listener
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT

      # Which listener for controller (internal Kafka communication)
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

      # Which listener for inter-broker communication
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      # ============================================
      # Topic Configuration (single-node setup)
      # ============================================
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

    # Healthcheck to know when Kafka is ready
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5

  # =============================================================================
  # PRODUCER - Stock Data (Synthetic)
  # =============================================================================
  stock-producer:
    build:
      context: ./src/producer
    container_name: stock-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
    profiles:
      - stock

  # =============================================================================
  # PRODUCER - Crypto Data (Real-time Coinbase)
  # =============================================================================
  crypto-producer:
    build:
      context: ./src/producer
    container_name: crypto-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
    command: ["python", "crypto_producer.py"]
    profiles:
      - crypto

  # =============================================================================
  # SPARK - Stream Processing
  # =============================================================================
  spark:
    # Official Apache Spark image (includes PySpark)
    image: apache/spark:3.5.0

    # Name the container
    container_name: spark

    # Run as root to allow VS Code/Cursor to install its server
    user: root

    # Wait for Kafka to be ready before starting
    depends_on:
      kafka:
        condition: service_healthy

    # Mount our code into the container
    # Format: "HOST_PATH:CONTAINER_PATH"
    volumes:
      - ./:/app    # Current folder -> /app in container

    # Set working directory inside container
    working_dir: /app

    environment:
      # ============================================
      # Spark Configuration
      # ============================================
      # Run Spark in local mode with 2 threads
      # Options: local[1], local[2], local[*] (all cores)
      SPARK_MASTER: local[2]

    # Keep container running so we can exec into it
    # (We'll manually run spark_consumer.py)
    command: sleep infinity

  # =============================================================================
  # CLICKHOUSE - Analytics Database
  # =============================================================================
  clickhouse:
    # Official ClickHouse image
    image: clickhouse/clickhouse-server:24.1

    # Name the container
    container_name: clickhouse

    # Expose ports
    ports:
      - "8123:8123"    # HTTP interface (for queries via curl/browser)
      - "9000:9000"    # Native TCP interface (for clients/drivers)

    # Store data persistently (survives container restarts)
    volumes:
      - clickhouse_data:/var/lib/clickhouse

    environment:
      # ============================================
      # ClickHouse Configuration
      # ============================================
      # Default user (for learning - in production use proper auth)
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1

    # Healthcheck to know when ClickHouse is ready
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5

# =============================================================================
# VOLUMES - Persistent Storage
# =============================================================================
volumes:
  clickhouse_data:    # Named volume for ClickHouse data
